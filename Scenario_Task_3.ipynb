{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SCENARIO :- 3"
      ],
      "metadata": {
        "id": "esRaGShT2M29"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are working in the AI team of an educational platform that wants to build a smart chatbot. The goal is to help students and literature enthusiasts ask questions about the content of a specific literary short story and get accurate, context-aware answers.\n",
        "\n",
        "Problem:\n",
        "\n",
        "Build a RAG-based (Retrieval-Augmented Generation) chatbot that can answer questions from a given PDF file.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Your Task:\n",
        "\n",
        "1.Extract text from the PDF using a Python library.\n",
        "\n",
        "2.Split the text into small chunks.\n",
        "\n",
        "3.Create embeddings for each chunk.\n",
        "\n",
        "4.Build a retriever to find relevant chunks for a user question.\n",
        "\n",
        "5.Generate answers by combining the user's question and the retrieved text using a language model.\n",
        "\n",
        "Expected Output:\n",
        "* A chatbot that can answer questions about the PDF\n",
        "* Answers should be relevant and based on the document\n",
        "*  Try evaluating with a few test questions\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "m7kLp85T2Mv8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BXUkRb3edqmo"
      },
      "outputs": [],
      "source": [
        "# pip install transformers PyPDF2 langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "import torch\n",
        "import textwrap\n",
        "\n",
        "\n",
        "def extract_pdf_text(pdf_path):\n",
        "\n",
        "  \"\"\"\n",
        "  Extracts and returns all text content from a PDF file.\n",
        "\n",
        "  Parameters:\n",
        "      pdf_path (str): The path to the PDF file.\n",
        "\n",
        "  Returns:\n",
        "      str: Combined text content from all pages in the PDF.\n",
        "  \"\"\"\n",
        "\n",
        "  reader = PdfReader(pdf_path)\n",
        "  text = \"\"\n",
        "  for page in reader.pages:\n",
        "\n",
        "    page_text = page.extract_text()\n",
        "    if page_text:\n",
        "      text += page_text\n",
        "  return text\n",
        "\n",
        "\n",
        "\n",
        "def split_into_chunks(text, max_tokens=300):\n",
        "  \"\"\"\n",
        "  Splits a long text string into smaller chunks.\n",
        "\n",
        "  Parameters:\n",
        "      text (str): The text to  split.\n",
        "      max_tokens (int): Maximum number of tokens.\n",
        "\n",
        "  Returns:\n",
        "      List[str]: A list of text chunks.\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  sentences = text.split('. ')\n",
        "  chunks, current_chunk = [], \"\"\n",
        "  for sentence in sentences:\n",
        "\n",
        "    if len(current_chunk.split()) + len(sentence.split()) < max_tokens:\n",
        "      current_chunk += sentence + \". \"\n",
        "    else:\n",
        "      chunks.append(current_chunk.strip())\n",
        "      current_chunk = sentence + \". \"\n",
        "\n",
        "  if current_chunk:\n",
        "    chunks.append(current_chunk.strip())\n",
        "  return chunks\n",
        "\n",
        "\n",
        "def load_flan_pipeline():\n",
        "  \"\"\"\n",
        "  Loads 'google/flan-t5-base' model.\n",
        "\n",
        "  Returns:\n",
        "      transformers.Pipeline: A text generation pipeline.\n",
        "  \"\"\"\n",
        "\n",
        "  model_name = \"google/flan-t5-base\"\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "  pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "  return pipe\n",
        "\n",
        "\n",
        "def find_relevant_chunk(query, chunks):\n",
        "  \"\"\"\n",
        "  Finds the most relevant text chunk based on the query.\n",
        "\n",
        "  Parameters:\n",
        "      query (str): The user input.\n",
        "      chunks (List[str]): A list of text chunks to search through.\n",
        "\n",
        "  Returns:\n",
        "      str: The chunk of text most relevant to the query.\n",
        "  \"\"\"\n",
        "\n",
        "  query_words = set(query.lower().split())\n",
        "  best_score = 0\n",
        "  best_chunk = \"\"\n",
        "  for chunk in chunks:\n",
        "    score = len(query_words.intersection(chunk.lower().split()))\n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_chunk = chunk\n",
        "  return best_chunk\n",
        "\n"
      ],
      "metadata": {
        "id": "0ZLXik1-d6TU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_pdf(pdf_path):\n",
        "\n",
        "  \"\"\"\n",
        "  Creates chat loop where user queries are answered\n",
        "  based on the content of the PDF.\n",
        "\n",
        "  Parameters:\n",
        "      pdf_path (str): Path to the PDF file.\n",
        "  \"\"\"\n",
        "\n",
        "  print(\"Reading PDF...\")\n",
        "  text = extract_pdf_text(pdf_path)\n",
        "  chunks = split_into_chunks(text)\n",
        "  flan_pipe = load_flan_pipeline()\n",
        "\n",
        "  print(\"Ask me anything.\\nType 'exit' to quit.\\n\\n\")\n",
        "  while True:\n",
        "    query = input(\"You: \")\n",
        "    if query.lower() in ['exit', 'quit']:\n",
        "\n",
        "      print(\"\\n\\nGOODBYE!\\n\\n\")\n",
        "      break\n",
        "    context = find_relevant_chunk(query, chunks)\n",
        "    prompt = f\"Context: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
        "    response = flan_pipe(prompt, max_new_tokens=100)[0]['generated_text']\n",
        "    print(\"\\nBot:\", textwrap.fill(response, width=80), \"\\n\")\n"
      ],
      "metadata": {
        "id": "aSgAWvKBd8HI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_with_pdf(\"/content/3_story.pdf\")"
      ],
      "metadata": {
        "id": "1C_03ohdfO3u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36f686e1-58ca-45e2-b450-04d9d4e95d3b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading PDF...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask me anything.\n",
            "Type 'exit' to quit.\n",
            "\n",
            "\n",
            "You: who wrote the story\n",
            "\n",
            "Bot: Gabriel Garcia Marquez \n",
            "\n",
            "You: what was frau frieda's talent\n",
            "\n",
            "Bot: oracular \n",
            "\n",
            "You: What was discovered inside the car that hit the hotel wall\n",
            "\n",
            "Bot: Frau Frieda \n",
            "\n",
            "You: Who was in the car?\n",
            "\n",
            "Bot: She was the housekeeper for the new Portuguese ambassador and his wife \n",
            "\n",
            "You: How many stories are there in the section \"I Sell My Dreams\" belongs to?\n",
            "\n",
            "Bot: three short stories and two long ones \n",
            "\n",
            "You: What unusual ring did the woman wear?\n",
            "\n",
            "Bot: a snake ring \n",
            "\n",
            "You: What happened to the woman in the car during the storm in Havana?\n",
            "\n",
            "Bot: she was extricated from the car encrusted in the wall of Havana Riviera Hotel \n",
            "\n",
            "You: Who raised Gabriel Garcia Marquez?\n",
            "\n",
            "Bot: his grandparents \n",
            "\n",
            "You: What prize did Gabriel Garcia Marquez win?\n",
            "\n",
            "Bot: the Nobel Prize in Literature \n",
            "\n",
            "You: What did Frau Frieda do at breakfast for the family she worked for?\n",
            "\n",
            "Bot: learn the immediate future of each of its members \n",
            "\n",
            "You: What unusual item did the woman in the car wear?\n",
            "\n",
            "Bot: a gold ring shaped like a serpent \n",
            "\n",
            "You: exit\n",
            "\n",
            "\n",
            "GOODBYE!\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vzFfv0tp0BgC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}